{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Jupyter Notebook for Visual Transformer Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Install Dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch\n",
    "%pip install torchvision\n",
    "%pip install \"numpy<2\"\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import OneCycleLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parse Dataset for Training**\n",
    "Parses the BreakHis dataset into a usable format for torch Dataset and DataLoader libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BreakHisDataset(Dataset):\n",
    "  def __init__(self, csv_file, root_dir, train=True, transform=None):\n",
    "    \n",
    "    self.data_frame = pd.read_csv(csv_file)\n",
    "    self.root_dir = root_dir\n",
    "    self.transform = transform\n",
    "\n",
    "    if train:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"train\"]\n",
    "    else:\n",
    "      self.data_frame = self.data_frame[self.data_frame['grp'].str.lower() == \"test\"]\n",
    "    \n",
    "    self.data_frame.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data_frame)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    row = self.data_frame.iloc[idx]\n",
    "    filename = row['filename']\n",
    "    \n",
    "    img_path = os.path.join(self.root_dir, filename)\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "  \n",
    "    lower_filename = filename.lower()\n",
    "    if \"adenosis\" in lower_filename:\n",
    "      label = 0\n",
    "    elif \"fibroadenoma\" in lower_filename:\n",
    "      label = 1\n",
    "    elif \"phyllodes_tumor\" in lower_filename:\n",
    "      label = 2\n",
    "    elif \"tubular_adenoma\" in lower_filename:\n",
    "      label = 3\n",
    "    elif \"ductal_carcinoma\" in lower_filename:\n",
    "      label = 4\n",
    "    elif \"lobular_carcinoma\" in lower_filename:\n",
    "      label = 5\n",
    "    elif \"mucinous_carcinoma\" in lower_filename:\n",
    "      label = 6\n",
    "    elif \"papillary_carcinoma\" in lower_filename:\n",
    "      label = 7\n",
    "    else:\n",
    "      raise ValueError(f\"Cannot determine label from filename: {filename}\")\n",
    "\n",
    "    if self.transform:\n",
    "      image = self.transform(image)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Patch Embedding**\n",
    "The class splits images into multiple different patches using Conv2d layers. This method is computationally efficient by utilizing a computer's GPU and parallel processes. Each patch is flattened into vectors for the model to interpret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "  def __init__(self, d_model, img_size, patch_size, n_channels):\n",
    "    super().__init__()\n",
    "    self.d_model = d_model\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.linear_project = nn.Conv2d(self.n_channels, self.d_model,\n",
    "                                    kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_project(x)\n",
    "    x = x.flatten(2)\n",
    "    x = x.transpose(1, 2)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Positional Encoding**\n",
    "#Check\n",
    "This class uses the sinusoidal position embedding developed in the paper [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762). Positional encoding helps ensure that images with scrambled patches are able to be differentiated by the model without hindering the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model, max_seq_length):\n",
    "    super().__init__()\n",
    "    self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "    pe = torch.zeros(max_seq_length, d_model)\n",
    "    for pos in range(max_seq_length):\n",
    "      for i in range(d_model):\n",
    "        if i % 2 == 0:\n",
    "          pe[pos][i] = np.sin(pos / (10000 ** (i / d_model)))\n",
    "        else:\n",
    "          pe[pos][i] = np.cos(pos / (10000 ** ((i - 1) / d_model)))\n",
    "    self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "  def forward(self, x):\n",
    "    tokens_batch = self.cls_token.expand(x.size(0), -1, -1)\n",
    "    x = torch.cat((tokens_batch, x), dim=1)\n",
    "    x = x + self.pe\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention Head**\n",
    "The class defines attention as the standard matrix of Queries (Q), Keys (K), and Values (V). The attention head is made into a learnable layer that the model can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "  def __init__(self, d_model, head_size):\n",
    "    super().__init__()\n",
    "    self.head_size = head_size\n",
    "    self.query = nn.Linear(d_model, head_size)\n",
    "    self.key = nn.Linear(d_model, head_size)\n",
    "    self.value = nn.Linear(d_model, head_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    Q = self.query(x)\n",
    "    K = self.key(x)\n",
    "    V = self.value(x)\n",
    "    attention = Q @ K.transpose(-2, -1)\n",
    "    attention = attention / (self.head_size ** 0.5)\n",
    "    attention = torch.softmax(attention, dim=-1)\n",
    "    out = attention @ V\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-Head Attention**\n",
    "This class creates a number of attention heads dependent on the number of specified heads. The number of heads must be a divisor of the number of channels in the model to ensure the proper sizing of the projected length of the tokens that the image provided is divided up into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "  def __init__(self, d_model, n_heads):\n",
    "    super().__init__()\n",
    "    self.head_size = d_model // n_heads\n",
    "    self.W_o = nn.Linear(d_model, d_model)\n",
    "    self.heads = nn.ModuleList([AttentionHead(d_model, self.head_size) for _ in range(n_heads)])\n",
    "\n",
    "  def forward(self, x):\n",
    "    out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "    out = self.W_o(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer Encoder**\n",
    "The transformer encoder utilizes the multi-head attention system to supply the model with information. This creates two skip connections, one with the multi-head attention system and the other with a multilayer perceptron, while also allowing for a dropout layer to help prevent overtraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "  def __init__(self, d_model, n_heads, r_mlp=4, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    self.ln1 = nn.LayerNorm(d_model)\n",
    "    self.mha = MultiHeadAttention(d_model, n_heads)\n",
    "    self.dropout1 = nn.Dropout(dropout_prob)\n",
    "    self.ln2 = nn.LayerNorm(d_model)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(d_model, d_model * r_mlp),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(dropout_prob),\n",
    "        nn.Linear(d_model * r_mlp, d_model),\n",
    "        nn.Dropout(dropout_prob)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    attn_out = self.mha(self.ln1(x))\n",
    "    x = x + self.dropout1(attn_out)\n",
    "    mlp_out = self.mlp(self.ln2(x))\n",
    "    x = x + mlp_out\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vision Transformer**\n",
    "This class creates a Vision Transformer model using the specified parameters and utilizes the previous classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "  def __init__(self, d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, dropout_prob=0.1):\n",
    "    super().__init__()\n",
    "    assert img_size[0] % patch_size[0] == 0 and img_size[1] % patch_size[1] == 0, \\\n",
    "        \"img_size dimensions must be divisible by patch_size dimensions\"\n",
    "    assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
    "\n",
    "    self.d_model = d_model\n",
    "    self.n_classes = n_classes\n",
    "    self.img_size = img_size\n",
    "    self.patch_size = patch_size\n",
    "    self.n_channels = n_channels\n",
    "    self.n_heads = n_heads\n",
    "    self.n_patches = (self.img_size[0] * self.img_size[1]) // (self.patch_size[0] * self.patch_size[1])\n",
    "    self.max_seq_length = self.n_patches + 1\n",
    "\n",
    "    self.patch_embedding = PatchEmbedding(self.d_model, self.img_size, self.patch_size, self.n_channels)\n",
    "    self.positional_encoding = PositionalEncoding(self.d_model, self.max_seq_length)\n",
    "    self.transformer_encoder = nn.Sequential(\n",
    "        *[TransformerEncoder(self.d_model, self.n_heads, r_mlp=4, dropout_prob=dropout_prob)\n",
    "          for _ in range(n_layers)]\n",
    "    )\n",
    "    self.classifier = nn.Linear(self.d_model, self.n_classes)\n",
    "\n",
    "  def forward(self, images):\n",
    "    x = self.patch_embedding(images)\n",
    "    x = self.positional_encoding(x)\n",
    "    x = self.transformer_encoder(x)\n",
    "    x = self.classifier(x[:, 0])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 16\n",
    "n_classes = 8  # e.g., nonbinary classification: 0 for adenosis, 1 for fibroadenoma, 2 for phyllodes_tumor, 3 for tubular_adenoma, 4 for ductal_carcinoma, 5 for lobular_carcinoma, 6 for mucinous_carcinoma, 7 for papillary_carcinoma\n",
    "img_size = (256, 256)\n",
    "patch_size = (16, 16)\n",
    "n_channels = 3\n",
    "n_heads = 4\n",
    "n_layers = 8\n",
    "batch_size = 256\n",
    "epochs = 200\n",
    "alpha = 0.001\n",
    "dropout_prob = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Image Transformations**\n",
    "The training transformations help to reduce overtraining by randomizing the images the model is trained on. The testing transformations help standardize the testing images to the correct, normalized images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(img_size, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.RandomRotation(degrees=15),\n",
    "    T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])\n",
    "\n",
    "\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(img_size),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=[0.7872, 0.6222, 0.7640], std=[0.1005, 0.1330, 0.0837])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = \"Folds.csv\"\n",
    "root_dir = \"./../BreaKHis_v1/\"\n",
    "\n",
    "train_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=True, transform=train_transform)\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=batch_size, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CUDA Loading**\n",
    "This portion of the code utilizes the Nvidia CUDA application to parallelize workloads while training on CUDA-enabled Nvidia GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.benchmark = True\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "  print(\"Using device:\", device, f\"({torch.cuda.get_device_name(device)})\")\n",
    "else:\n",
    "  print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Necessities**\n",
    "The transformer is created and saved to the allocated device. This training loop utilizes the AdamW optimizer to help maintain model stability and improve accuracy by decoupling weight decay from the loss function. Additionally, OneCycleLR is used to help the model properly converge by adjusting the learning rate over time depending on the specified number of epochs. Cross-entry loss is used since this is a non-binary classifation model and to help reward the model for predicting correctly with a high confidence rating and penalizing it for a wrong prediction with a high confindence rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = VisionTransformer(d_model, n_classes, img_size, patch_size, n_channels, n_heads, n_layers, dropout_prob).to(device)\n",
    "optimizer = AdamW(transformer.parameters(), lr=alpha, weight_decay=1e-4)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=0.01, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**\n",
    "This is a standard training loop that utilizes the previous transformer, optimizer, scheduler, and criterion. It tracks the time for each epoch and saves the model after every 10 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "  start = time.time()\n",
    "  transformer.train()\n",
    "  training_loss = 0.0\n",
    "  for inputs, labels in train_loader:\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    optimizer.zero_grad()\n",
    "    outputs = transformer(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    training_loss += loss.item()\n",
    "  end = time.time()\n",
    "  curr_lr = scheduler.get_last_lr()\n",
    "  print(f'Epoch {epoch + 1}/{epochs} loss: {training_loss / len(train_loader):.3f} time: {end-start:.2f} sec lr: {curr_lr}')\n",
    "  if ((epoch+1) % 10) == 0:\n",
    "    model_scripted = torch.jit.script(transformer)\n",
    "    model_scripted.save(f\"./checkpoints/checkpoint:{epoch+1}-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}-{dropout_prob}.pth\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Loop**\n",
    "This is a standard training loop that outputs the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = BreakHisDataset(csv_file=csv_file, root_dir=root_dir, train=False, transform=test_transform)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=batch_size, num_workers=8, pin_memory=True)\n",
    "\n",
    "\n",
    "def test_model(model, test_loader, device):\n",
    "  model.eval()\n",
    "  correct = 0\n",
    "  total = 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "      images, labels = images.to(device), labels.to(device)\n",
    "      outputs = model(images)\n",
    "      _, predicted = torch.max(outputs, dim=1)\n",
    "      total += labels.size(0)\n",
    "      correct += (predicted == labels).sum().item()\n",
    "\n",
    "  accuracy = 100 * correct / total\n",
    "  print(f'\\nModel Accuracy: {accuracy:.2f}%')\n",
    "  return accuracy\n",
    "\n",
    "test_accuracy = test_model(transformer, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save Final Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_scripted = torch.jit.script(transformer)\n",
    "model_scripted.save(f\"./models/nonbinary/{test_accuracy:.2f}%-{d_model}-{n_classes}-{img_size}-{patch_size}-{n_channels}-{n_heads}-{n_layers}-{batch_size}-{epochs}-{alpha}-{dropout_prob}.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
